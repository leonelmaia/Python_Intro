{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=15%><img src=\"./img/UGA.png\"></img></td>\n",
    "<td><center><h1>Refresher Course on Matrix Analysis and Optimization</h1></center></td>\n",
    "<td width=15%><a href=\"http://www.iutzeler.org\" style=\"font-size: 16px; font-weight: bold\">Franck Iutzeler</a> <a href=\"https://ljk.imag.fr/membres/Jerome.Malick/\" style=\"font-size: 16px; font-weight: bold\">Jerome Malick</a><br/> Fall. 2018 </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><div id=\"top\"></div>\n",
    "\n",
    "<center><a style=\"font-size: 40pt; font-weight: bold\">Chap. 2 - Refresher Course </a></center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "# ``2. Numerical Optimization``\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#style\"><b>Package check and Styling</b></a><br/><br/><b>Outline</b><br/><br/>\n",
    "&nbsp;&nbsp;&nbsp; a) <a href=\"#OGrad\"> the Gradient Method </a><br/>&nbsp;&nbsp;&nbsp; b) <a href=\"#ORef\"> Application to Regression </a><br/>&nbsp;&nbsp;&nbsp; c) <a href=\"#OAda\"> an Adaptive Gradient Algorithm </a><br/>&nbsp;&nbsp;&nbsp; d) <a href=\"#OCla\"> Application to Classification </a><br/>&nbsp;&nbsp;&nbsp; e) <a href=\"#ONew\"> Newton's Algorithm </a><br/>&nbsp;&nbsp;&nbsp; f) <a href=\"#OFur\"> To go Further </a><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"OGrad\"> a) the Gradient Method </a>  \n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n",
    "\n",
    "\n",
    "We consider two $\\mathbb{R}^2 \\to \\mathbb{R}$ convex functions with the same global minimizer $(3,1)$ but quite different *shapes* and see how this impacts the performance of gradient-based algorithms. The considered functions $f$ and $g$ and their 3D are:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>\\begin{array}{rrcll}\n",
    "f: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & 4 (x_1-3)^2 + 2(x_2-1)^2\n",
    "\\end{array}</th>\n",
    "<th> \\begin{array}{rrcll}\n",
    "g: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & \\log( 1 + \\exp(4 (x_1-3)^2 ) + \\exp( 2(x_2-1)^2 ) ) - \\log(3) .\n",
    "\\end{array} </th>\n",
    "</tr>\n",
    "<td> <img src=\"img/simple.png\" alt=\"f\" />  </td>\n",
    "<td> <img src=\"img/harder.png\" alt=\"f\" /> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For convenience, we provide custom plotting functions in <tt>lib/custom_plot_lib.py</tt>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.custom_plot_lib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 1:</b> Observe how is written the function <tt>f</tt> that return $f(x)$ from input vector $x$. Observe the 3D plot and level plot with <tt>custom_3dplot...</tt> and <tt>level_plot...</tt>. Do the same for  function <tt>g</tt>.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Function f\n",
    "    \"\"\"\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    return 4*(x1-3)**2+2*(x2-1)**2\n",
    "\n",
    "f_plot_param  = {'x1_min' : -0.5, 'x1_max' : 5.5,\n",
    "                 'x2_min' : -0.5, 'x2_max' : 5.5,\n",
    "                 'nb_points' : 200,\n",
    "                 'v_min' : 0, 'v_max' : 80, 'levels' : [0.5,1,2,5,10,15],\n",
    "                 'title' : 'f: a simple function' }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_3dplot( f, f_plot_param )\n",
    "level_plot( f, f_plot_param )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return 0 ## To complete\n",
    "\n",
    "\n",
    "\n",
    "g_plot_param  = {'x1_min' : -0.5, 'x1_max' : 5.5,\n",
    "                 'x2_min' : -0.5, 'x2_max' : 5.5,\n",
    "                 'nb_points' : 500,\n",
    "                 'v_min' : 0, 'v_max' : 100, 'levels' : [0.5,1,2,5,10,15],\n",
    "                 'title' : 'g: a harder function' }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_3dplot( g, g_plot_param )\n",
    "#level_plot( g, g_plot_param )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 2:</b>  Fill the function <tt>f_grad</tt> that return $\\nabla f(x)$ from input vector $x$. Do the same for  function <tt>g_grad</tt>.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_grad(x): # ...................................\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    g = np.array( [ 0.0  ,  0.0  ] )\t\n",
    "    return g\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_grad(x): # ...................................\n",
    "    g = np.array( [ 0.0  ,  0.0  ] )\t\n",
    "    return g\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 3:</b>  implement a constant stepsize gradient method <tt>gradient_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX )</tt> that takes:\n",
    "<ul>\n",
    "<li> <tt>f</tt> and <tt>f_grad</tt>: respectively functions and gradient simulators;</li>\n",
    "<li> <tt>x0</tt>: starting point;</li>\n",
    "<li> <tt>step</tt>: a stepsize;</li>\n",
    "<li> <tt>PREC</tt> and <tt>ITE_MAX</tt>: stopping criteria for sought precision and maximum number of iterations;</li>\n",
    "</ul>\n",
    "\n",
    "and return <tt>x</tt>, the final value, and <tt>x_tab</tt>, the matrix of all vectors stacked vertically.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX ): # .......................................\n",
    "    x = np.copy(x0)\n",
    "    x_tab = np.copy(x)\n",
    "\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 4:</b> Test your gradient descent function on $f$ and $g$: i) Verify that the final point is close to the sought minimizer $(3,1)$; ii) observe the behavior of the iterates with <tt>level_points_plot</tt>. Change the stepsize and give the values for which the algorithm (i) diverges and (ii) oscillates. Compare with theoretical limits by computing the Lipschitz constant of the gradients.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step =  0 # .........................\n",
    "PREC = 1e-5 # .........................\n",
    "ITE_MAX = 10 # .........................\n",
    "x0 = np.array([0,0]) # .........................\n",
    "\n",
    "x,x_tab = gradient_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX )\n",
    "level_points_plot( f , x_tab , f_plot_param )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step =  0 # .........................\n",
    "PREC = 1e-5 # .........................\n",
    "ITE_MAX = 10 # .........................\n",
    "x0 = np.array([0,0]) # .........................\n",
    "\n",
    "x,x_tab = gradient_algorithm(g , g_grad , x0 , step , PREC , ITE_MAX )\n",
    "level_points_plot( g , x_tab , g_plot_param )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"ORef\"> b) Application to Regression </a>  \n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n",
    "\n",
    "\n",
    "We now get back to the problem of predicting the final grade of a student from various features treated in the Matrix part of the course.\n",
    "\n",
    "We remind that mathematically, from the $m_{learn}  \\times (n+1)$  *learning matrix* $A_{learn}$\n",
    "($m_{learn} = 300$, $n=27$)  comprising of the features values of each training student in line, and the vector of the values of the target features $b_{learn}$;  we seek a size-$(n+1)$ *regression vector* that minimizes the squared error between  $A_{learn} x$ and $b_{learn}$. This problem boils down to the following least square problem:\n",
    "$$ \\min_{x\\in\\mathbb{R}^{n+1}} s(x) =  \\frac{1}{2} \\|  A_{learn} x - b_{learn} \\|_2^2 . $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# File reading\n",
    "dat_file = np.load('data/student.npz')\n",
    "A_learn = dat_file['A_learn']\n",
    "b_learn = dat_file['b_learn']\n",
    "A_test = dat_file['A_test']\n",
    "b_test = dat_file['b_test']\n",
    "\n",
    "m = 395 # number of read examples (total:395)\n",
    "n = 27 # features\n",
    "m_learn = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 1:</b> Construct the suitable function $s$ and gradient simulator as in the previous section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 2:</b> Compute the Lipschitz constant of the gradient of $s$. Find a solution to the minimization of $s$ using your gradient algorithm. Compare with Numpy's Least Square routine.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 3:</b> Generate a random Gaussian matrix/vector couple $A,b$ with increasing size. Create simulators to compare the execution time of constant stepsize gradien and pseudo-inverse computation \\emph{via} SVD on the least squares problem $\\min_x \\|Ax-b\\|_2^2$. Notably change the *shape* of $A$ from *tall* (nb. of rows $>\\!>$ nb. of cols.) to *fat* (nb. of rows $<\\!<$ nb. of cols.).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"OAda\"> c) an Adaptive Gradient Algorithm </a>  \n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n",
    "\n",
    "The *Rosenbrock* function $r$ writes \n",
    "<table>\n",
    "<tr>\n",
    "<th>\\begin{array}{rrcll}\n",
    "r: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  &  (1-x_1)^2 + 100(x_2-x_1^2)^2 .\n",
    "\\end{array}</th>\n",
    "</tr>\n",
    "<td> <img src=\"img/rosenbrock.png\" alt=\"f\" />  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 1:</b> Fill the functions <tt>r</tt> that return $r(x)$ from input vector $x$; and <tt>r_grad</tt> that return $\\nabla r(x)$ from input vector $x$. Observe the 3D plot and level plot of the function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Definition of function r\n",
    "def r(x):\n",
    "    \"\"\" Rosenbrock.\"\"\"\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    return (1-x1)**2+100*(x2-x1**2)**2\n",
    "\n",
    "\n",
    "\n",
    "r_plot_param  = {'x1_min' : -1.5, 'x1_max' : 1.55,\n",
    "                 'x2_min' : -0.2, 'x2_max' : 1.5,\n",
    "                 'nb_points' : 200,\n",
    "                 'v_min' : 0, 'v_max' : 120, 'levels' : [0.05,1,5,15,50,200],\n",
    "                 'title' : 'r: Rosenbrock function' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_3dplot( r,r_plot_param)\n",
    "level_plot( r, r_plot_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_grad(x): # .........................\n",
    "    g = np.array( [ 0.0  ,  0.0  ] )\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 2:</b> Try to minimize $r$ using your constant stepsize gradient function <tt>gradient_algorithm</tt>. Can you find a stepsize for which the algorithm converges?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 3:</b> Implement an *adaptive* stepsize gradient method <tt>gradient_adaptive_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX )</tt> that takes the same inputs and returns the same as the gradient method but implements a *stepsize adaptation method*. For instance, one can use this rule:\n",
    "\\begin{align*}\n",
    "\\mathbf{if} f(x_{k+1})>f(x_k)&:\\\\\n",
    "x_{k+1} &= x_k\\\\\n",
    "step &= step/2\n",
    "\\end{align*}\n",
    "which halves the stepsize if a gradient step makes the functional value increase.<br/>\n",
    "\n",
    "Test your method on $r$: i) Verify that the final point is close to the sought minimizer $(1,1)$; ii) observe the behavior of the iterates with <tt>level_points_plot</tt>. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step =  0 # .........................\n",
    "PREC = 1e-5 # .........................\n",
    "ITE_MAX = 10 # .........................\n",
    "x0 = np.array([0,0]) # .........................\n",
    "\n",
    "x,x_tab = gradient_adaptive_algorithm(r , r_grad , x0 , step , PREC , ITE_MAX )\n",
    "#level_points_plot( r , x_tab , r_plot_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"OCla\"> d) Application to Classification </a>  \n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n",
    "\n",
    "\n",
    "Binary classification is another popular problem in machine learning. Instead of predicting a numerical value, the goal is now to classify the student into two classes: $+1$ -- *pass* i.e. final grade $\\geq 10$; and $-1$ -- *fail*. To this purpose, we create a class vector $c_{learn}$  from the observation vector $b_{learn}$ by simply setting $c_{learn}(i) = +1 $ if  $b_{learn}(i)\\geq10$ and $-1$ otherwise. Then, the most common approach is to minimize the logistic loss regularized by a squared norm:\n",
    "\\begin{equation}\n",
    "\\min_{x\\in\\mathbb{R}^{n+1}}  \\ell(x) = \\sum_{i=1}^{m_{learn} } \\log\\left( 1 + \\exp\\left( -c_{learn}(i) a_i^{\\mathrm{T}}  x   \\right) \\right)  + \\frac{1}{m}\\|x\\|^2\n",
    "\\end{equation}\n",
    "where $a^\\mathrm{T}_i$ is the $i$-th row of $A_{learn}$.\n",
    "\n",
    "Then, from a solution $x^\\star$ of this problem, one can classify a new example, represented by its feature vector $a$, as such: the quantity $p(a) = \\frac{1}{1+\\exp(- a^\\mathrm{T} x^\\star)}$ estimates the probability of belonging to class $1$; thus, one can decide class $+1$ if for instance $ p(a) \\geq 0.5$; otherwise, decide class $-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 1:</b> Compute the gradient of $q(t)  = \\log(1+\\exp(t))$. Is the function is convex? Deduce that $\\ell$ is convex and its gradient.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 2:</b> Construct the suitable function and gradient simulators in order to use your <tt>gradient_adaptive_algorithm</tt> to minimize $\\ell$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 3:</b> From a final point of the optimization algorithm above, generate a decision vector corresponding to the testing set $A_{test}$. Evaluate the classification error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"ONew\"> e) Newton's Algorithm </a>  \n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 1:</b> Fill the function <tt>f_grad_hessian</tt> that return $\\nabla f(x)$ and $H_f(x)$ from input vector $x$. Same thing for $g$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_grad_hessian(x): # ...................................\n",
    "    g = np.array( [ 0.0  ,  0.0  ] )\n",
    "    H = np.array(  [ ( 0.0 , 0.0 )  ,  ( 0.0 , 0.0 )  ]  )\n",
    "    return g,H\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_grad_hessian(x): # ...................................\n",
    "    g = np.array( [ 0.0  ,  0.0  ] )\n",
    "    H = np.array(  [ ( 0.0 , 0.0 )  ,  ( 0.0 , 0.0 )  ]  )\n",
    "    return g,H\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 2:</b> Implement Newton's method in <tt>newton_algorithm(f , f_grad_hessian , x0 , PREC , ITE\\_MAX )</tt> that takes as an input:\n",
    "<ul>\n",
    "<li> <tt>f</tt> and <tt>f_grad_hessian</tt>: respectively functions and gradient + Hessian simulators;\n",
    "<li> <tt>x0</tt>: starting point;</li>\n",
    "<li> <tt>PREC</tt> and <tt>ITE_MAX</tt>: stopping criteria for sought precision and maximum number of iterations;</li>\n",
    "</ul>\n",
    "<tt>x</tt> the final value, and <tt>x_tab</tt>, the matrix of all vectors stacked vertically.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_algorithm(f , f_grad_hessian , x0 , PREC , ITE_MAX ): # .......................................\n",
    "    x = np.copy(x0)\n",
    "    x_tab = np.copy(x)\n",
    "\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 3:</b> Test your method on $f$ and $g$: i) Verify that the final point is close to the sought minimizer $(3,1)$; ii) observe the behavior of the iterates with <tt>level_points_plot</tt>.  \n",
    "\n",
    "\n",
    "Compare graphically constant stepsize gradient and Newton's algorithms with <tt>level_2points_plot</tt>. <br/>\n",
    "\n",
    "\n",
    "Newton's algorithm should take exactly one iteration to converge for function $f$. Why so? Is it the case for function $g$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question 4:</b> Compare Newton method with the adaptive gradient in the case of the classification problem\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"OFur\"> f) To go Further </a>  \n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n",
    "\n",
    "\n",
    "We introduce two functions:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>\\begin{array}{rrcll}\n",
    "t: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  & (0.6 x_1 + 0.2 x_2)^2 \\left((0.6 x_1 + 0.2 x_2)^2 - 4 (0.6 x_1 + 0.2 x_2)+4\\right) \\\\\n",
    "&            &         & + (-0.2 x_1 + 0.6 x_2)^2\n",
    "\\end{array}</th>\n",
    "<th> \\begin{array}{rrcll}\n",
    "p: & \\mathbb{R}^2 & \\to &\\mathbb{R}\\\\\n",
    "& (x_1,x_2) & \\mapsto  &  \\left| x_1-3 \\right|  + 2\\left| x_2-1\\right| \n",
    "\\end{array} </th>\n",
    "</tr>\n",
    "<td> <img src=\"img/poly.png\" alt=\"f\" />  </td>\n",
    "<td> <img src=\"img/two_pits.png\" alt=\"f\" /> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"exo\"> <b>Question:</b> Test adaptive gradient methods on these functions from different starting points. What do you observe?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div id=\"style\"></div>\n",
    "### Package Check and Styling\n",
    "\n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.notebook_setting as nbs\n",
    "\n",
    "packageList = ['IPython', 'numpy', 'scipy', 'matplotlib', 'cvxopt']\n",
    "nbs.packageCheck(packageList)\n",
    "\n",
    "nbs.cssStyling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
